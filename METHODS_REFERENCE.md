# é•¿å°¾ä¸å¹³è¡¡å­¦ä¹ å®éªŒæŒ‡å—

æœ¬æ–‡æ¡£æŒ‰ç…§è®ºæ–‡å®éªŒè®¾è®¡çš„é€»è¾‘ç»„ç»‡ï¼ŒåŒ…å«åŸºå‡†å®éªŒã€å¯¹æ¯”å®éªŒã€æ¶ˆèå®éªŒçš„å®Œæ•´å‘½ä»¤è¡Œã€‚

**MoE-LTSEI** æ˜¯æœ¬æ–‡æå‡ºçš„æ–¹æ³•ã€‚

---

## ğŸ“‹ å®éªŒè®¾è®¡æ¦‚è§ˆ

| å®éªŒç±»å‹ | ç›®çš„ | ç« èŠ‚ |
|----------|------|------|
| **åŸºå‡†å®éªŒ** | å»ºç«‹ Baselineï¼ŒéªŒè¯é•¿å°¾é—®é¢˜å­˜åœ¨ | 1.1 |
| **å•é˜¶æ®µå¯¹æ¯”å®éªŒ** | å¯¹æ¯”ç°æœ‰å•é˜¶æ®µæ–¹æ³• | 2.1 - 2.3 |
| **ä¸¤é˜¶æ®µå¯¹æ¯”å®éªŒ** | å¯¹æ¯”ç°æœ‰ä¸¤é˜¶æ®µæ–¹æ³• | 3.1 - 3.3 |
| **æ¶ˆèå®éªŒ** | éªŒè¯ MoE-LTSEI å„ç»„ä»¶è´¡çŒ® | 4 |
| **MoE-LTSEI (Ours)** | æœ¬æ–‡æå‡ºçš„å®Œæ•´æ–¹æ³• | 5 |

---

## 1. åŸºå‡†å®éªŒ (Baseline)

### 1.1 æ ‡å‡†äº¤å‰ç†µ (æ— ä»»ä½•é•¿å°¾å¤„ç†)

ä½œä¸ºæ‰€æœ‰å¯¹æ¯”å®éªŒçš„åŸºå‡†çº¿ã€‚

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

---

## 2. å•é˜¶æ®µå¯¹æ¯”å®éªŒ

### 2.1 é‡åŠ æƒæ–¹æ³• (Re-weighting)

#### Focal Loss
> Lin et al., "Focal Loss for Dense Object Detection", **ICCV 2017**
> 
> é€šè¿‡ $(1-p_t)^\gamma$ è°ƒåˆ¶å› å­èšç„¦éš¾åˆ†ç±»æ ·æœ¬

```bash
python main.py loss.name=FocalLoss loss.focal_gamma=2.0 sampling.name=none stage2.enabled=false
```

#### Class-Balanced Loss (CB)
> Cui et al., "Class-Balanced Loss Based on Effective Number of Samples", **CVPR 2019**
> 
> åŸºäºæœ‰æ•ˆæ ·æœ¬æ•° $E_n = (1-\beta^n)/(1-\beta)$ è®¡ç®—ç±»åˆ«æƒé‡

```bash
python main.py loss.name=ClassBalancedLoss loss.cb_beta=0.9999 sampling.name=none stage2.enabled=false
```

#### LDAM (Label-Distribution-Aware Margin)
> Cao et al., "Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss", **NeurIPS 2019**
> 
> ä¸ºå°‘æ•°ç±»è®¾ç½®æ›´å¤§è¾¹ç•Œ $\Delta_j = C / n_j^{1/4}$

```bash
python main.py loss.name=LDAMLoss loss.ldam_max_margin=0.5 loss.ldam_scale=1.0 loss.ldam_drw_start=0 sampling.name=none stage2.enabled=false
```

#### LDAM + DRW (Deferred Re-Weighting)
> åŒä¸Šï¼Œé…åˆå»¶è¿Ÿé‡åŠ æƒç­–ç•¥

```bash
python main.py loss.name=LDAMLoss loss.ldam_max_margin=0.5 loss.ldam_scale=1.0 loss.ldam_drw_start=160 loss.ldam_reweight_power=0.25 sampling.name=none stage2.enabled=false
```

---

### 2.2 å…ˆéªŒè°ƒæ•´æ–¹æ³• (Logit Adjustment)

#### Balanced Softmax
> Ren et al., "Balanced Meta-Softmax for Long-Tailed Visual Recognition", **NeurIPS 2020**
> 
> Logits åŠ å…¥ç±»åˆ«å…ˆéªŒ $\tilde{z}_c = z_c + \log \pi_c$

```bash
python main.py loss.name=BalancedSoftmaxLoss sampling.name=none stage2.enabled=false
```

#### Logit Adjustment (LA)
> Menon et al., "Long-tail Learning via Logit Adjustment", **ICLR 2021**
> 
> å¯è°ƒå¼ºåº¦çš„å…ˆéªŒè°ƒæ•´ $\tilde{z}_c = z_c + \tau \log \pi_c$

```bash
python main.py loss.name=LogitAdjustmentLoss loss.logit_tau=1.0 sampling.name=none stage2.enabled=false
```

---

### 2.3 é‡é‡‡æ ·æ–¹æ³• (Re-sampling)

#### é€†é¢‘ç‡é‡‡æ · (Inverse Frequency)
> ç»å…¸é‡é‡‡æ ·æ–¹æ³•ï¼Œé‡‡æ ·æ¦‚ç‡ $p_c \propto 1/n_c$

```bash
python main.py loss.name=CrossEntropy sampling.name=inv_freq stage2.enabled=false
```

#### ç±»åˆ«å‡åŒ€é‡‡æ · (Class-Uniform)
> æ¯ä¸ªç±»åˆ«é‡‡æ ·æ¦‚ç‡ç›¸ç­‰

```bash
python main.py loss.name=CrossEntropy sampling.name=class_uniform stage2.enabled=false
```

#### å¹³æ–¹æ ¹é‡‡æ · (Square-Root)
> é‡‡æ ·æ¦‚ç‡ $p_c \propto 1/\sqrt{n_c}$ï¼Œä»‹äºåŸå§‹åˆ†å¸ƒä¸å‡åŒ€åˆ†å¸ƒä¹‹é—´

```bash
python main.py loss.name=CrossEntropy sampling.name=sqrt stage2.enabled=false
```

---

## 3. ä¸¤é˜¶æ®µå¯¹æ¯”å®éªŒ

### 3.1 Decoupling æ–¹æ³•

#### cRT (Classifier Re-Training)
> Kang et al., "Decoupling Representation and Classifier for Long-Tailed Recognition", **ICLR 2020**
> 
> Stage-1: æ ‡å‡†è®­ç»ƒå­¦ä¹ è¡¨å¾ï¼›Stage-2: å†»ç»“ backboneï¼Œå¹³è¡¡é‡‡æ ·é‡è®­åˆ†ç±»å™¨

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.epochs=100 stage2.lr=0.1 stage2.optimizer=SGD stage2.loss=CrossEntropy stage2.sampler=class_uniform stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### Ï„-norm (Weight Normalization)
> åŒä¸Šè®ºæ–‡ï¼Œå¯¹åˆ†ç±»å™¨æƒé‡åš Ï„ èŒƒæ•°å½’ä¸€åŒ–

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=false stage3.mode=tau_norm stage3.tau_norm=1.0
```

#### LWS (Learnable Weight Scaling)
> åŒä¸Šè®ºæ–‡ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„ç±»åˆ«æƒé‡ç¼©æ”¾

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.epochs=100 stage2.lr=0.1 stage2.loss=CrossEntropy stage2.sampler=class_uniform stage2.freeze_bn=true
```

---

### 3.2 ä»£ä»·æ•æ„Ÿä¸¤é˜¶æ®µæ–¹æ³•

#### cRT + Cost-Sensitive CE
> Stage-2 ä½¿ç”¨ä»£ä»·æ•æ„ŸæŸå¤±ï¼Œä»£ä»·æƒé‡ $w_c \propto 1/n_c$

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.epochs=100 stage2.lr=0.1 stage2.optimizer=SGD stage2.loss=CostSensitiveCE stage2.cost_strategy=auto stage2.sampler=progressive_power stage2.alpha_start=1.0 stage2.alpha_end=0.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### cRT + Cost-Sensitive CE (sqrt)
> ä»£ä»·æƒé‡ $w_c \propto 1/\sqrt{n_c}$ï¼Œæ›´æ¸©å’Œçš„é‡åŠ æƒ

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.epochs=100 stage2.lr=0.1 stage2.optimizer=SGD stage2.loss=CostSensitiveCE stage2.cost_strategy=sqrt stage2.sampler=progressive_power stage2.alpha_start=1.0 stage2.alpha_end=0.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

---

### 3.3 Label Smoothing ä¸¤é˜¶æ®µæ–¹æ³•

#### cRT + LOS (Label Over-Smoothing)
> ICLR 2025ï¼Œä½¿ç”¨æå¤§ Label Smoothing (Îµâ‰ˆ0.98) ä½¿ç›®æ ‡åˆ†å¸ƒæ¥è¿‘å‡åŒ€

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.epochs=100 stage2.lr=0.1 stage2.optimizer=SGD stage2.loss=LOS stage2.los_smoothing=0.98 stage2.sampler=class_uniform stage2.freeze_bn=true stage2.warmup_epochs=5
```

---

## 4. æ¶ˆèå®éªŒ (Ablation Study)

éªŒè¯ MoE-LTSEI å„ç»„ä»¶çš„è´¡çŒ®ã€‚

### 4.1 Backbone å¯¹æ¯”

#### å•ä¸“å®¶: ConvNetADSB
```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

#### å•ä¸“å®¶: ResNet1D
```bash
python main.py model.name=ResNet1D loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

#### å•ä¸“å®¶: DilatedTCN
```bash
python main.py model.name=DilatedTCN loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

#### å•ä¸“å®¶: FrequencyDomainExpert
```bash
python main.py model.name=FrequencyDomainExpert loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

#### MoE ç»“æ„ (æ— é•¿å°¾å¤„ç†)
```bash
python main.py model.name=MixtureOfExpertsConvNet loss.name=CrossEntropy sampling.name=none stage2.enabled=false
```

---

### 4.2 æŸå¤±å‡½æ•°æ¶ˆè

#### MoE + æ ‡å‡† CE (æ— è¾¹ç•Œ)
```bash
python main.py model.name=MixtureOfExpertsConvNet loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.loss=CrossEntropy stage2.sampler=class_uniform stage2.freeze_bn=true stage2.epochs=100 stage2.lr=0.1
```

#### MoE + Class-Balanced Loss
```bash
python main.py model.name=MixtureOfExpertsConvNet loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.loss=ClassBalancedLoss stage2.sampler=class_uniform stage2.freeze_bn=true stage2.epochs=100 stage2.lr=0.1
```

#### MoE + Cost-Sensitive CE
```bash
python main.py model.name=MixtureOfExpertsConvNet loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=crt stage2.loss=CostSensitiveCE stage2.cost_strategy=auto stage2.sampler=progressive_power stage2.alpha_start=1.0 stage2.alpha_end=0.0 stage2.freeze_bn=true stage2.epochs=100 stage2.lr=0.1
```

---

### 4.3 MoE-LTSEI ç»„ä»¶æ¶ˆè

#### MoE-LTSEI w/o Gate Loss (Î»_gate=0)
> ç§»é™¤é—¨æ§ç›‘ç£æŸå¤±

```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=0.0 stage2.moe_loss.lambda_lb=0.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### MoE-LTSEI w/o Adaptive Margin (margin_m0=0)
> ç§»é™¤è‡ªé€‚åº”è¾¹ç•Œ

```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.0 stage2.moe_loss.lambda_gate=1.0 stage2.moe_loss.lambda_lb=0.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### MoE-LTSEI w/o Difficulty Weighting (diff_gamma=0)
> ç§»é™¤éš¾åº¦åŠ æƒ

```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.diff_gamma=0.0 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### MoE-LTSEI with Load Balance (Î»_lb=0.01)
> æ·»åŠ è´Ÿè½½å‡è¡¡æ­£åˆ™åŒ–

```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.moe_loss.lambda_lb=0.01 stage2.freeze_bn=true stage2.warmup_epochs=5
```

---

### 4.4 ä¸“å®¶æ•°é‡æ¶ˆè

#### 2 Experts
```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=2 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### 3 Experts (é»˜è®¤)
```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

#### 4 Experts
```bash
python main.py model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=4 stage2.moe.gate_hidden=128 stage2.moe_loss.scale=30.0 stage2.moe_loss.beta=0.999 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true stage2.warmup_epochs=5
```

---

### 4.5 ä¸å¹³è¡¡æ¯”æ¶ˆè

#### IR = 10
```bash
python main.py data.imbalance_ratio=10 model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe_loss.scale=30.0 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true
```

#### IR = 50
```bash
python main.py data.imbalance_ratio=50 model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe_loss.scale=30.0 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true
```

#### IR = 100 (é»˜è®¤)
```bash
python main.py data.imbalance_ratio=100 model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe_loss.scale=30.0 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true
```

#### IR = 200
```bash
python main.py data.imbalance_ratio=200 model.name=ConvNetADSB loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe_loss.scale=30.0 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true
```

---

## 5. MoE-LTSEI (Ours) - å®Œæ•´æ–¹æ³•

### 5.1 å®Œæ•´é…ç½®

**Stage-1**: æ ‡å‡† CE è®­ç»ƒï¼Œå­¦ä¹ é€šç”¨è¡¨å¾

**Stage-2**: MoE åˆ†ç±»å™¨å¤´ + è‡ªé€‚åº”è¾¹ç•ŒæŸå¤± + é—¨æ§ç›‘ç£

```bash
python main.py \
  model.name=ConvNetADSB \
  model.dropout=0.1 \
  model.use_attention=true \
  loss.name=CrossEntropy \
  sampling.name=none \
  training.epochs=300 \
  training.lr=0.1 \
  training.optimizer=SGD \
  stage2.enabled=true \
  stage2.mode=moe_ltsei \
  stage2.epochs=100 \
  stage2.lr=0.1 \
  stage2.optimizer=SGD \
  stage2.loss=MoELTSEILoss \
  stage2.moe.num_experts=3 \
  stage2.moe.gate_hidden=128 \
  stage2.moe.gate_dropout=0.1 \
  stage2.moe.normalize_features=true \
  stage2.moe_loss.scale=30.0 \
  stage2.moe_loss.beta=0.999 \
  stage2.moe_loss.margin_m0=0.35 \
  stage2.moe_loss.margin_m1=0.2 \
  stage2.moe_loss.margin_gamma=0.5 \
  stage2.moe_loss.diff_gamma=2.0 \
  stage2.moe_loss.diff_alpha=1.0 \
  stage2.moe_loss.lambda_gate=1.0 \
  stage2.moe_loss.lambda_lb=0.0 \
  stage2.sampler=progressive_power \
  stage2.alpha_start=1.0 \
  stage2.alpha_end=0.0 \
  stage2.freeze_bn=true \
  stage2.warmup_epochs=5 \
  data.imbalance_ratio=100
```

### 5.2 ç®€æ´ç‰ˆæœ¬ (ä½¿ç”¨é»˜è®¤å€¼)

```bash
python main.py loss.name=CrossEntropy sampling.name=none stage2.enabled=true stage2.mode=moe_ltsei stage2.epochs=100 stage2.lr=0.1 stage2.loss=MoELTSEILoss stage2.moe.num_experts=3 stage2.moe_loss.scale=30.0 stage2.moe_loss.margin_m0=0.35 stage2.moe_loss.lambda_gate=1.0 stage2.freeze_bn=true
```

---

## é™„å½•: é€šç”¨å‚æ•°è¯´æ˜

### A. æ•°æ®ç›¸å…³
| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `data.imbalance_ratio` | ä¸å¹³è¡¡æ¯” (å¤´ç±»/å°¾ç±») | 10, 50, 100, 200 |
| `data.batch_size` | æ‰¹é‡å¤§å° | 128, 256 |

### B. è®­ç»ƒç›¸å…³
| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `training.epochs` | Stage-1 è®­ç»ƒè½®æ¬¡ | 200, 300 |
| `training.lr` | Stage-1 å­¦ä¹ ç‡ | 0.01, 0.1 |
| `training.optimizer` | ä¼˜åŒ–å™¨ | SGD, Adam, AdamW |
| `gpus` | GPU ID | 0, "0,1" |

### C. Stage-2 ç›¸å…³
| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `stage2.enabled` | æ˜¯å¦å¯ç”¨ Stage-2 | true, false |
| `stage2.mode` | æ¨¡å¼ | crt, finetune, moe_ltsei |
| `stage2.epochs` | Stage-2 è®­ç»ƒè½®æ¬¡ | 50, 100 |
| `stage2.lr` | Stage-2 å­¦ä¹ ç‡ | 0.01, 0.1 |
| `stage2.freeze_bn` | æ˜¯å¦å†»ç»“ BN | true, false |

### D. MoE-LTSEI ä¸“ç”¨å‚æ•°
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `stage2.moe.num_experts` | ä¸“å®¶æ•°é‡ | 3 |
| `stage2.moe.gate_hidden` | é—¨æ§éšè—å±‚ç»´åº¦ | 128 |
| `stage2.moe_loss.scale` | Logit ç¼©æ”¾å› å­ | 30.0 |
| `stage2.moe_loss.beta` | æœ‰æ•ˆæ ·æœ¬æ•° Î² | 0.999 |
| `stage2.moe_loss.margin_m0` | åŸºç¡€è¾¹ç•Œ | 0.35 |
| `stage2.moe_loss.diff_gamma` | éš¾åº¦è°ƒåˆ¶ Î³ | 2.0 |
| `stage2.moe_loss.lambda_gate` | é—¨æ§æŸå¤±æƒé‡ | 1.0 |
| `stage2.moe_loss.lambda_lb` | è´Ÿè½½å‡è¡¡æƒé‡ | 0.0 |

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Focal Loss**: Lin et al., ICCV 2017
2. **Class-Balanced Loss**: Cui et al., CVPR 2019
3. **LDAM**: Cao et al., NeurIPS 2019
4. **Balanced Softmax**: Ren et al., NeurIPS 2020
5. **Logit Adjustment**: Menon et al., ICLR 2021
6. **Decoupling (cRT/Ï„-norm/LWS)**: Kang et al., ICLR 2020
7. **LOS**: ICLR 2025

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´: 2026-01-12*
